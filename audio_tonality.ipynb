{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 256618,
          "sourceType": "datasetVersion",
          "datasetId": 107620
        }
      ],
      "dockerImageVersionId": 30839,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "audio tonality",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "\n",
        "# import kagglehub\n",
        "# uwrfkaggler_ravdess_emotional_speech_audio_path = kagglehub.dataset_download('uwrfkaggler/ravdess-emotional-speech-audio')\n",
        "\n",
        "# print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "BVMGLj789Vr9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "tei6FHB29VsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "dataset_path = \"/kaggle/input/ravdess-emotional-speech-audio\"\n",
        "\n",
        "# Словарь эмоций\n",
        "emotions_map = {\n",
        "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
        "    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
        "}\n",
        "# 01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised\n",
        "\n",
        "# Функция загрузки аудиофайлов и меток\n",
        "def load_data(dataset_path):\n",
        "    audio_files = []\n",
        "    labels = []\n",
        "    for root, _, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.wav'):\n",
        "                emotion_code = file.split('-')[2]  # Эмоция в имени файла\n",
        "                if emotion_code in emotions_map:\n",
        "                    labels.append(emotions_map[emotion_code])\n",
        "                    audio_files.append(os.path.join(root, file))\n",
        "    return audio_files, labels\n",
        "\n",
        "# Загружаем данные\n",
        "audio_files, labels = load_data(dataset_path)\n",
        "\n",
        "# Кодируем эмоции\n",
        "# le = LabelEncoder()\n",
        "# labels_encoded = le.fit_transform(labels)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T20:33:56.537748Z",
          "iopub.execute_input": "2025-01-28T20:33:56.538099Z",
          "iopub.status.idle": "2025-01-28T20:34:04.12912Z",
          "shell.execute_reply.started": "2025-01-28T20:33:56.538058Z",
          "shell.execute_reply": "2025-01-28T20:34:04.127947Z"
        },
        "id": "ri00-gWC9VsE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Извлечение признаков из аудио:**\n",
        "librosa: конвертируем аудио в спектрограммы MFCC (Mel-Frequency Cepstral Coefficients)"
      ],
      "metadata": {
        "id": "ErpZ1HVJ9VsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем предобученную Wav2Vec 2.0"
      ],
      "metadata": {
        "id": "zttQXuYe9VsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "\n",
        "# Загружаем предобученную модель Wav2Vec 2.0\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "# Функция для извлечения признаков\n",
        "def extract_wav2vec_features(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=16000)  # Загружаем аудио\n",
        "    inputs = processor(y, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        features = model(inputs.input_values).last_hidden_state\n",
        "    return features.mean(dim=1)  # Усредняем по времени\n",
        "\n",
        "# Извлекаем признаки из всех файлов\n",
        "wav2vec_features = [extract_wav2vec_features(path) for path in audio_files]\n",
        "features_tensor = torch.cat(wav2vec_features)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-28T20:36:53.685896Z",
          "iopub.execute_input": "2025-01-28T20:36:53.686445Z",
          "iopub.status.idle": "2025-01-28T20:37:20.414714Z",
          "shell.execute_reply.started": "2025-01-28T20:36:53.686411Z",
          "shell.execute_reply": "2025-01-28T20:37:20.412619Z"
        },
        "id": "z9CenT2-9VsG",
        "outputId": "4a860e8e-3771-4b3a-acdf-309d349fba9e",
        "colab": {
          "referenced_widgets": [
            "c334ad6d881e4b0dbd84bceab3a10052",
            "3f3bb859e9bb4feca06d94379c039125",
            "637113d31d664779989f2ac5c3cb96ea",
            "dc400999936546aea2f53e456b74933d",
            "c5fde4757a634882b4bbbef31b19cc72",
            "4b8b4b02575c4873989340b9b034705e"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c334ad6d881e4b0dbd84bceab3a10052"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f3bb859e9bb4feca06d94379c039125"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "637113d31d664779989f2ac5c3cb96ea"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc400999936546aea2f53e456b74933d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5fde4757a634882b4bbbef31b19cc72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b8b4b02575c4873989340b9b034705e"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Создание классификатора эмоций"
      ],
      "metadata": {
        "id": "xEdU1NFM9VsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Здесь должен быть классификатор"
      ],
      "metadata": {
        "id": "8lPbuKtt9lpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель обучена на английских данных, а нам нужна русская речь\n",
        "\n",
        "Fine-tuning на русских данных Golos"
      ],
      "metadata": {
        "id": "7Iogmf5v9VsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# открываем данные Golos\n",
        "def read_file_list(manifest):\n",
        "    manifest_path, _ = os.path.split(manifest)\n",
        "    files, texts = [], []\n",
        "    with open(manifest, \"r\") as input_file:\n",
        "        for line in input_file:\n",
        "            as_dict = json.loads(line.rstrip('\\n'))\n",
        "            files.append(os.path.join(manifest_path, as_dict[\"audio_filepath\"]))\n",
        "            texts.append(as_dict[\"text\"])\n",
        "    return files, texts"
      ],
      "metadata": {
        "trusted": true,
        "id": "BZdSGEjg9VsJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Заменяем аудиофайлы RAVDESS на Golos\n",
        "audio_files, labels = read_file_list .................\n",
        "\n",
        "# Извлекаем признаки с помощью Wav2Vec 2.0\n",
        "wav2vec_features = [extract_wav2vec_features(path) for path in audio_files]\n",
        "features_tensor = torch.cat(wav2vec_features)\n",
        "\n",
        "# Перекодируем метки эмоций\n",
        "labels_encoded = le.transform(labels)\n",
        "labels_tensor = torch.tensor(labels_encoded)\n",
        "\n",
        "# Повторно обучаем классификатор\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9jqDKQYA9VsK"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}